<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="article"/>
<title>ardatasc.github.io</title>
<link rel="icon" href="../../favicon.png" type="image/x-icon">
<link rel="preconnect" href="https://fonts.googleapis.com/">
<link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
<link rel="preconnect" href="https://fonts.googleapis.com/">
<link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap.css" rel="stylesheet">
<link rel="preconnect" href="https://fonts.googleapis.com/">
<link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&amp;display=swap" rel="stylesheet">
<link rel="stylesheet" href="../../theme.min.7363202e6ba39c3b4caca8b218f6f6631256ed7a41cd2f8bf46550651b07cb31.css" integrity="">
</head>
<body>
  <div class="header">
  <div class="header-content">
  <div class="header-content-title">
  ardatasc
  </div>
  <div class="header-content-nav">
  <div class="header-content-nav-item">
  <a href="../../c.html">content</a>
  </div>
  <div class="header-content-nav-item">
  <a href="../../about.html">about</a>
  </div>
  </div>
  </div>
  </div>
<div class="content-container">
<div class="content"><h1 id="highly-available-control-plane-with-kubeadm">Highly Available Control Plane with kubeadm</h1>
<p>Kubernetes 1.14 introduced an
<a href="https://web.archive.org/web/20240228165447/https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#kubeadm-maturity">ALPHA</a>
feature for dynamically adding master nodes to a cluster. This prevents the need
to copy certificates and keys among nodes relieving additional orchestration and
complexity in the bootstrapping process. In this post we will dive into how it
works.</p>
<b><a href="https://web.archive.org/web/20240228165447/https://youtu.be/27v36t-3afQ">Click here to watch the video version of this content.</a></b>
<h2 id="how-it-works">How It Works</h2>
<p><code>kubeadm init</code> initializes a Kubernetes cluster by standing up a single master.
After running the command, you end up with the following.</p>
<a href="https://web.archive.org/web/20240228165447/https://octetz.s3.us-east-2.amazonaws.com/master.png" target="octetz-img">
<img src="https://web.archive.org/web/20240228165447im_/https://octetz.s3.us-east-2.amazonaws.com/master.png" width="400">
</a>
<blockquote>
<p>This represents the default. Through additional configuration, <code>kubeadm init</code>
can behave differently, such as reusing an existing etcd cluster.</p>
</blockquote>
<p>Conventionally, after installing a CNI plugin, users copy PKI information across
2 more master nodes and run a <code>kubeadm</code> command to add new control plane nodes.
This results in a 3 node control plane.</p>
<p>1.14 introduced the <code>--upload-certs</code> flag to the <code>kubeadm init</code> command. As
detailed in the
<a href="https://web.archive.org/web/20240228165447/https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/20190122-Certificates-copy-for-kubeadm-join--control-plane.md">KEP</a>,
it has the following impact.</p>
<ol>
<li>
<p>Creates an encryption key on the host.</p>
</li>
<li>
<p>Encrypts certificates and keys with the encryption key.</p>
</li>
<li>
<p>Adds encrypted data to the <code>kubeadm-certs</code> secret in the <code>kube-system</code>
namespace.</p>
</li>
<li>
<p>Links the <code>kubeadm-certs</code> secret to a kubeadm token with a <strong>1 hour TTL</strong>.</p>
</li>
</ol>
<p>Taking the example above, if we run the following command, it will result in the
new diagram below.</p>
<pre tabindex="0"><code>kubeadm init --experimental-upload-certs
</code></pre>
<a href="https://web.archive.org/web/20240228165447/https://octetz.s3.us-east-2.amazonaws.com/master2.png" target="octetz-img">
<img src="https://web.archive.org/web/20240228165447im_/https://octetz.s3.us-east-2.amazonaws.com/master2.png" width="600">
</a>
<p>When the kubeadm token expires, so does the <code>kubeadm-certs</code> secret. Also,
whenever the <code>init phase upload-certs</code> is run, a new encryption key is created.
Ensuring that if <code>kube-apiserver</code> is compromised during the adding of a master
node, the secret is encrypted and meaningless to the attacker. This example
demonstrates running <code>--experimental-upload-certs</code> during cluster bootstrap. It
is also possible to tap into phases, using <code>kubeadm init phase upload-certs</code> to
achieve the above on an existing master. This will be detailed in the
walkthrough below.</p>
<p>Lastly, it is important to note the token generated during <code>upload-certs</code> is
only a proxy used to bind a TTL to the <code>kubeadm-cert</code> secret. You still need a
conventional <code>kubeadm</code> token to join a control plane. This is the same token you
would need to join a worker.</p>
<p>To add another control plane (master) node, a user can run the following
command.</p>
<pre tabindex="0"><code>kubeadm join ${API_SERVER_PROXY_IP}:${API_SERVER_PROXY_PORT} \
    --experimental-control-plane \
    --certificate-key=${ENCRYPTION_KEY} \
    --token ${KUBEADM_TOKEN} \
    --discovery-token-ca-cert-hash ${APISERVER_CA_CERT_HASH}
</code></pre><h2 id="walkthrough-creating-the-ha-control-plane">Walkthrough: Creating the HA Control Plane</h2>
<p>This walkthrough will guide you to creating a new Kubernetes cluster with a 3
node control plane. It will demonstrate joining a control plane right after
bootstrap and how to add another control plane node after the bootstrap tokens
have expired.</p>
<ol>
<li>
<p>Create 4 hosts (vms, baremetal, etc).</p>
<blockquote>
<p>These hosts will be referred to as <code>loadbalancer</code>, <code>master0</code>, <code>master1</code>,
and <code>master2</code>.</p>
</blockquote>
<blockquote>
<p>master hosts may need 2 vCPUs and 2GB of RAM available. You can get around
these requirements during testing by ignoring pre-flight checks. See the
kubeadm documentation for more details.</p>
</blockquote>
</li>
<li>
<p>Record the host IPs for later use.</p>
</li>
</ol>
<h3 id="setup-the-load-balancer">Setup the Load Balancer</h3>
<p>In this section, we'll run a simple NGINX load balancer to provide a single
endpoint for our control plane. This load balancer example is not meant for
production scenarios.</p>
<ol>
<li>
<p>SSH to the <code>loadbalancer</code> host.</p>
</li>
<li>
<p>Create the directory <code>/etc/nginx</code>.</p>
<pre tabindex="0"><code>mkdir /etc/nginx
</code></pre></li>
<li>
<p>Add and edit the file <code>/etc/nginx/nginx.conf</code>.</p>
<pre tabindex="0"><code>vim /etc/nginx/nginx.conf
</code></pre></li>
<li>
<p>Inside the file, add the following configuration.</p>
<pre tabindex="0"><code>events { }

stream {
    upstream stream_backend {
        least_conn;
        # REPLACE WITH master0 IP
        server 192.168.122.160:6443;
        # REPLACE WITH master1 IP
        server 192.168.122.161:6443;
        # REPLACE WITH master2 IP
        server 192.168.122.162:6443;
    }

    server {
        listen        6443;
        proxy_pass    stream_backend;
        proxy_timeout 3s;
        proxy_connect_timeout 1s;
    }

}
</code></pre></li>
<li>
<p>Alter each line above with a <code>REPLACE</code> comment above it.</p>
</li>
<li>
<p>Start NGINX.</p>
<pre tabindex="0"><code>docker run --name proxy \
    -v /etc/nginx/nginx.conf:/etc/nginx/nginx.conf:ro \
    -p 6443:6443 \
    -d nginx
</code></pre></li>
<li>
<p>Verify you can reach NGINX at its address.</p>
<pre tabindex="0"><code>curl 192.168.122.170
</code></pre><p>output:</p>
<pre tabindex="0"><code>curl: (52) Empty reply from server
</code></pre></li>
</ol>
<h3 id="install-kubernetes-binaries-on-master-hosts">Install Kubernetes Binaries on Master Hosts</h3>
<ol>
<li>
<p>Complete all pre-requisites and installation on master nodes.</p>
<p>a. See: <a href="https://web.archive.org/web/20240228165447/https://kubernetes.io/docs/setup/independent/install-kubeadm">https://kubernetes.io/docs/setup/independent/install-kubeadm</a>.</p>
</li>
</ol>
<h3 id="initialize-the-cluster-and-examine-certificates">Initialize the Cluster and Examine Certificates</h3>
<ol>
<li>
<p>SSH to the <code>master0</code> host.</p>
</li>
<li>
<p>Create the directory <code>/etc/kubernetes/kubeadm</code></p>
<pre tabindex="0"><code>mkdir /etc/kubernetes/kubeadm
</code></pre></li>
<li>
<p>Create and edit the file <code>/etc/kubernetes/kubeadm/kubeadm-config.yaml</code>.</p>
<pre tabindex="0"><code>vim /etc/kubernetes/kubeadm/kubeadm-config.yaml
</code></pre></li>
<li>
<p>Add the following configuration.</p>
<pre tabindex="0"><code>apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
kubernetesVersion: stable
# REPLACE with `loadbalancer` IP
controlPlaneEndpoint: &#34;192.168.122.170:6443&#34;
networking:
  podSubnet: 192.168.0.0/18
</code></pre></li>
<li>
<p>Alter the line with a REPLACE comment above it.</p>
</li>
<li>
<p>Initialize the cluster with <code>upload-certs</code> and <code>config</code> specified.</p>
<pre tabindex="0"><code>kubeadm init \
    --config=/etc/kubernetes/kubeadm/kubeadm-config.yaml \
    --experimental-upload-certs
</code></pre></li>
<li>
<p>Record the output regarding joining control plane nodes for later use.</p>
<p>output:</p>
<pre tabindex="0"><code>You can now join any number of the control-plane node running the following command on each as root:

kubeadm join 192.168.122.170:6443 --token nmiqmn.yls76lcyxg2wt36c \
--discovery-token-ca-cert-hash sha256:5efac16c86e5f2ed6b20c6dbcbf3a9daa5bf75aa604097dbf49fdc3d1fd5ff7d \
--experimental-control-plane --certificate-key 828fc83b950fca2c3bda129bcd0a4ffcd202cfb1a30b36abb901de1a3626a9df
</code></pre><blockquote>
<p>Note the <code>certificate-key</code> that enables decrypting the kubeadm certs secret.</p>
</blockquote>
</li>
<li>
<p>As your user, run the recommended kubeconfig commands for <code>kubectl</code> access.</p>
<pre tabindex="0"><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre></li>
<li>
<p>Examine the <code>kubeadm-cert</code> secret in the <code>kube-system</code> namespace.</p>
<pre tabindex="0"><code>kubectl get secrets -n kube-system kubeadm-certs -o yaml
</code></pre><blockquote>
<p>You should see certs for etcd, kube-apiserver, and service accounts.</p>
</blockquote>
</li>
<li>
<p>Under <code>ownerReferences</code>, examine the <code>name</code>.</p>
<pre tabindex="0"><code>name: bootstrap-token-cwb9ra
</code></pre><blockquote>
<p>This correlates to an ephemeral kubeadm token. When that token expires, so does this secret.</p>
</blockquote>
</li>
<li>
<p>List available tokens with <code>kubeadm</code>.</p>
<pre tabindex="0"><code>kubeadm token list
</code></pre><p>output:</p>
<p>TOKEN TTL EXPIRES USAGES DESCRIPTION <br>
cwb9ra.gegoj2eqddaf3yps 1h 2019-03-26T19:38:18Z <none> Proxy for managing TTL for the kubeadm-certs secret
nmiqmn.yls76lcyxg2wt36c 23h 2019-03-27T17:38:18Z authentication,signing <none></p>
<pre tabindex="0"><code>
&gt; Note that `cwb9ra` is the owner reference in the above step. This is
&gt; **not** a join token, instead a proxy that enables ttl on `kubeadm-certs`.
&gt; We still need to use the `nmiqmn` token when joining.
</code></pre></li>
<li>
<p>Install <code>calico</code> CNI-plugin with a pod CIDR matching the <code>podSubnet</code> configured above.</p>
<pre tabindex="0"><code>kubectl apply -f https://gist.githubusercontent.com/joshrosso/ed1f5ea5a2f47d86f536e9eee3f1a2c2/raw/dfd95b9230fb3f75543706f3a95989964f36b154/calico-3.5.yaml
</code></pre></li>
<li>
<p>Verify 1 node is <code>Ready</code>.</p>
<pre tabindex="0"><code>kubectl get nodes
</code></pre><p>output:</p>
<pre tabindex="0"><code>NAME              STATUS   ROLES    AGE   VERSION
192-168-122-160   Ready    master   79m   v1.14.0
</code></pre></li>
<li>
<p>Verify <code>kube-system</code> pods are <code>Running</code>.</p>
<pre tabindex="0"><code>kubectl get pods -n kube-system
</code></pre><p>output:</p>
<pre tabindex="0"><code>NAME                                      READY   STATUS    RESTARTS   AGE
calico-node-mphpw                         1/1     Running   0          58m
coredns-fb8b8dccf-c6s9q                   1/1     Running   0          80m
coredns-fb8b8dccf-mxzrm                   1/1     Running   0          80m
etcd-192-168-122-160                      1/1     Running   0          79m
kube-apiserver-192-168-122-160            1/1     Running   0          79m
kube-controller-manager-192-168-122-160   1/1     Running   0          79m
kube-proxy-dpxhx                          1/1     Running   0          80m
kube-scheduler-192-168-122-160            1/1     Running   0          79m
</code></pre></li>
</ol>
<h3 id="add-the-second-master">Add the Second Master</h3>
<ol>
<li>
<p>SSH to the <code>master1</code> host.</p>
</li>
<li>
<p>Run the recorded join command from the previous section.</p>
<pre tabindex="0"><code>kubeadm join 192.168.122.170:6443 --token nmiqmn.yls76lcyxg2wt36c \
--discovery-token-ca-cert-hash sha256:5efac16c86e5f2ed6b20c6dbcbf3a9daa5bf75aa604097dbf49fdc3d1fd5ff7d \
--experimental-control-plane \
--certificate-key 828fc83b950fca2c3bda129bcd0a4ffcd202cfb1a30b36abb901de1a3626a9df
</code></pre></li>
<li>
<p>After completion, verify there are now 2 nodes.</p>
<pre tabindex="0"><code>kubectl get nodes
</code></pre><p>output:</p>
<pre tabindex="0"><code>NAME              STATUS   ROLES    AGE   VERSION
192-168-122-160   Ready    master   22m   v1.14.0
192-168-122-161   Ready    master   34s   v1.14.0
</code></pre></li>
<li>
<p>Verify new pods have been created.</p>
<pre tabindex="0"><code>kubectl get pods -n kube-system
</code></pre><p>output:</p>
<pre tabindex="0"><code>NAME                                      READY   STATUS    RESTARTS   AGE
calico-node-cq5nt                         1/1     Running   0          60s
calico-node-spn5w                         1/1     Running   0          13m
coredns-fb8b8dccf-r9sc8                   1/1     Running   0          22m
coredns-fb8b8dccf-wlcm4                   1/1     Running   0          22m
etcd-192-168-122-160                      1/1     Running   0          21m
etcd-192-168-122-161                      1/1     Running   0          59s
kube-apiserver-192-168-122-160            1/1     Running   0          21m
kube-apiserver-192-168-122-161            1/1     Running   0          59s
kube-controller-manager-192-168-122-160   1/1     Running   0          21m
kube-controller-manager-192-168-122-161   1/1     Running   0          60s
kube-proxy-tflhf                          1/1     Running   0          60s
kube-proxy-vthjr                          1/1     Running   0          22m
kube-scheduler-192-168-122-160            1/1     Running   0          22m
kube-scheduler-192-168-122-161            1/1     Running   0          59s
</code></pre></li>
</ol>
<h3 id="add-the-third-master-with-new-tokens">Add the Third Master with New Tokens</h3>
<p>This section joins the third and final master. However, we will first delete all
existing <code>kubeadm</code> tokens. This approach demonstrates how you could add masters
when the Kubernetes cluster is already running.</p>
<ol>
<li>
<p>On an existing master, list all tokens.</p>
<pre tabindex="0"><code>kubeadm token list
</code></pre></li>
<li>
<p>Delete all existing tokens.</p>
<pre tabindex="0"><code>kubeadm token delete cwb9ra.gegoj2eqddaf3yps
kubeadm token delete nmiqmn.yls76lcyxg2wt36c
</code></pre><blockquote>
<p>Now the previously recorded join command will not work as the
kubeadm-certs secret has expired and been deleted, the encryption key is
no longer valid, and the join token will not work.</p>
</blockquote>
</li>
<li>
<p>Create a new token with a 10 minute TTL.</p>
<pre tabindex="0"><code>kubeadm token create --ttl 10m --print-join-command
</code></pre><p>output:</p>
<pre tabindex="0"><code>kubeadm join 192.168.122.170:6443 \
    --token xaw58o.0fjg0xp0ohpucwhr \
    --discovery-token-ca-cert-hash sha256:5efac16c86e5f2ed6b20c6dbcbf3a9daa5bf75aa604097dbf49fdc3d1fd5ff7d
</code></pre><blockquote>
<p>Note the IP above must reflect your <code>loadbalancer</code> host.</p>
</blockquote>
</li>
<li>
<p>Run the <code>upload-certs</code> phase of <code>kubeadm init</code>.</p>
<pre tabindex="0"><code>kubeadm init phase upload-certs --experimental-upload-certs
</code></pre><p>output:</p>
<pre tabindex="0"><code>[upload-certs] Storing the certificates in ConfigMap &#34;kubeadm-certs&#34; in the &#34;kube-system&#34; Namespace
[upload-certs] Using certificate key:
9555b74008f24687eb964bd90a164ecb5760a89481d9c55a77c129b7db438168
</code></pre></li>
<li>
<p>SSH to the <code>master2</code> host.</p>
</li>
<li>
<p>Use the outputs from the previous steps to run a control-plane join command.</p>
<pre tabindex="0"><code>kubeadm join 192.168.122.170:6443 \
    --experimental-control-plane \
    --certificate-key 9555b74008f24687eb964bd90a164ecb5760a89481d9c55a77c129b7db438168 \
    --token xaw58o.0fjg0xp0ohpucwhr \
    --discovery-token-ca-cert-hash sha256:5efac16c86e5f2ed6b20c6dbcbf3a9daa5bf75aa604097dbf49fdc3d1fd5ff7d
</code></pre></li>
<li>
<p>After completion, verify there are now 3 nodes.</p>
<pre tabindex="0"><code>kubectl get nodes
</code></pre><p>output:</p>
<pre tabindex="0"><code>NAME              STATUS   ROLES    AGE     VERSION
192-168-122-160   Ready    master   50m     v1.14.0
192-168-122-161   Ready    master   28m     v1.14.0
192-168-122-162   Ready    master   3m30s   v1.14.0
</code></pre></li>
</ol>
<h2 id="summary">Summary</h2>
<p>I hope you found this post helpful in understanding HA bootstrapping with
kubeadm. Checkout the
<a href="https://web.archive.org/web/20240228165447/https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/20190122-Certificates-copy-for-kubeadm-join--control-plane.md">KEP</a>
and
<a href="https://web.archive.org/web/20240228165447/https://kubernetes.io/docs/setup/independent/high-availability">documentation</a>
for more features and configuration. I am really looking forward to the
hardening of this feature and perhaps what it can mean for installation methods
such as <code>kubespray</code> and Cluster API. Special thanks to everyone in
sig-cluster-lifecycle who developed, reviewed, and documented this awesome
feature.</p>
</div>
<div class="content-toc">
<h3>Contents</h3>
<nav id="TableOfContents">
<ul>
<li><a href="index.html#how-it-works">How It Works</a></li>
<li><a href="index.html#walkthrough-creating-the-ha-control-plane">Walkthrough: Creating the HA Control Plane</a>
<ul>
<li><a href="index.html#setup-the-load-balancer">Setup the Load Balancer</a></li>
<li><a href="index.html#install-kubernetes-binaries-on-master-hosts">Install Kubernetes Binaries on Master Hosts</a></li>
<li><a href="index.html#initialize-the-cluster-and-examine-certificates">Initialize the Cluster and Examine Certificates</a></li>
<li><a href="index.html#add-the-second-master">Add the Second Master</a></li>
<li><a href="index.html#add-the-third-master-with-new-tokens">Add the Third Master with New Tokens</a></li>
</ul>
</li>
<li><a href="index.html#summary">Summary</a></li>
</ul>
</nav>
</div>
</div>
</body>
</html>
<!--
     FILE ARCHIVED ON 16:54:47 Feb 28, 2024 AND RETRIEVED FROM THE
     INTERNET ARCHIVE ON 03:20:20 Sep 10, 2024.
     JAVASCRIPT APPENDED BY WAYBACK MACHINE, COPYRIGHT INTERNET ARCHIVE.

     ALL OTHER CONTENT MAY ALSO BE PROTECTED BY COPYRIGHT (17 U.S.C.
     SECTION 108(a)(3)).
-->
<!--
playback timings (ms):
  captures_list: 1.283
  exclusion.robots: 0.024
  exclusion.robots.policy: 0.011
  esindex: 0.012
  cdx.remote: 28.553
  LoadShardBlock: 135.711 (3)
  PetaboxLoader3.datanode: 82.621 (4)
  PetaboxLoader3.resolve: 124.92 (2)
  load_resource: 127.296
-->
